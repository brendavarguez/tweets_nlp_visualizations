{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import requests\n",
    "import unidecode\n",
    "import unicodedata\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loadinng credenttials as environmen variables\n",
    "load_dotenv('/home/bmartin/Documents/github_repos/tweets_nlp_visualizations/credentials.env', \n",
    "            override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting twitter credentials\n",
    "twitter_key = os.environ.get('api_key')\n",
    "twitter_secret_key = os.environ.get('api_secret_key')\n",
    "bearer_token = os.environ.get('bearer_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20221101_20_05'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get city coordinates\n",
    "#geolocator = Nominatim(user_agent = 'bmartin')\n",
    "\n",
    "# Get current date\n",
    "today = dt.datetime.today()\n",
    "#today = today.strftime(\"%Y-%m-%d %H:%M\")\n",
    "today = today.strftime(\"%Y%m%d_%H_%M\")\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Twitter API\n",
    "\n",
    "### 1.1 About the Twitter API\n",
    "\n",
    "The Twitter API can be used to retrieve and analyze data, as well as engage with the conversation on Twitter. It provides access to a variety of different resources including:\n",
    "- Tweets\n",
    "- Users\n",
    "- Direct Messages\n",
    "- Lists\n",
    "- Trends\n",
    "- Media\n",
    "- Places\n",
    "\n",
    "The Twitter API currently consists of two supported versions, as well as different access tiers. \n",
    "- **Standard v1.1**: The legacy standard endpoints provide access to the following resources with the standard v1.1 offerings.\n",
    "    \n",
    "    - Get Tweet timelines\n",
    "    - Curate a collection of Tweets\n",
    "    - Filter realtime Tweets\n",
    "    - Sample realtime Tweets\n",
    "    - Manage and pull public account information\n",
    "    - Get trends near a location\n",
    "    - Get locations with trending topics\n",
    "    - Get information about a place    \n",
    "    \n",
    "    \n",
    "- **Twitter API v2 Early Access**: A new Twitter API is being build with a modern and more sustainable foundation as well as an improved developer experience. The first endpoints are now available within Early Access, and enable users to listen to and analyze the public conversation. Additional endpoints, features, and access levels will be released soon.\n",
    "    \n",
    "    - Ability to request specific objects and fields.\n",
    "    - New and more detailed data objects\n",
    "    - Advanced metrics return in Tweets (including impressions, video views, user profile and URL clicks)\n",
    "    - Insights on Tweet topics with annotations (filter by topic using `entity` and `context` operators)\n",
    "    - Improved conversation tracking\n",
    "    - Academic Research product track (grants free access to full-archive search)\n",
    "    - High confidence spam filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Retrieve data\n",
    "### 1.2.1 Search Tweets\n",
    "\n",
    "The [search endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent) returns Tweets from the last seven days that match a search query. Parameters are listed below:\n",
    "- `query`(required): rule for matching Tweets.\n",
    "- `expansions`: Expansions enable users to request additional data objects that relate to the originally returned Tweets.\n",
    "- `max_results`: The maximum number of results to be returned.\n",
    "- `next_token`: This parameter is used to get the next 'page' of results. \n",
    "- `place.fields`: Enables to select specific place fields that will be delivered in each returned Tweet. \n",
    "- `tweet.fields`: Enables to select specific Tweet fields that will be delivered in each returned Tweet object.\n",
    "- `user.fields`: Enables to select specific user fields that will be delivered in each returned Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(query, bearer_token = bearer_token, next_token = None):    \n",
    "    \"\"\"\n",
    "    Function to request tweets according to a specific query.\n",
    "    \n",
    "    Inputs:\n",
    "        - query: A string that will be used to find tweets.\n",
    "                 Tweets must match this string to be returned.\n",
    "        - bearer_token: Security token from Twitter API.\n",
    "        - next_token: ID of the next page that matches the specified query.\n",
    "        \n",
    "    Outputs: Dictionary (json type) with the requested data.  \n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    \n",
    "    # end point\n",
    "    url = f\"https://api.twitter.com/2/tweets/search/recent?query={query}&\"\n",
    "\n",
    "    params = {\n",
    "        # select specific Tweet fields from each returned Tweet object\n",
    "        'tweet.fields': 'text,created_at,lang,possibly_sensitive', # public_metrics\n",
    "        \n",
    "        # maximum number of search results to be returned (10 - 100)\n",
    "        'max_results': 100,\n",
    "        \n",
    "        # additional data that relate to the originally returned Tweets\n",
    "        'expansions': 'author_id,referenced_tweets.id,geo.place_id',\n",
    "        \n",
    "        # select specific place fields \n",
    "        \"place.fields\": 'country,full_name,name',\n",
    "        \n",
    "        # select specific user fields\n",
    "        \"user.fields\": 'location',\n",
    "        \n",
    "        # get the next page of results.\n",
    "        \"next_token\": next_token,\n",
    "    }\n",
    "    \n",
    "    # request\n",
    "    response = requests.get(url = url, params = params, headers = headers)\n",
    "\n",
    "    # verify successfull request\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "        \n",
    "    else:\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'includes', 'meta'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"world cup\"\n",
    "\n",
    "# search term\n",
    "search_tweet = search_tweets(query = query)\n",
    "\n",
    "# 4 main keys\n",
    "search_tweet.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate some data\n",
    "\n",
    "After having requested tweets data, some dataframes were generated with different data.\n",
    "\n",
    "- **tweets**: Pandas dataframe with information about tweets. The data from this dataframe is the required to perform text classification as well as to generate some visualizations. Columns:\n",
    "    - `text`: Tweet content.\n",
    "    - `lang`: Tweets' original language. Although some tweets are not in English, during the pre-processing process tweets are translated from the origin language to English.\n",
    "    - `possibly_sensitive`: Boolean. Specifies if the tweet might be sensitive. \n",
    "    - `tweet_id`: Tweet's unique identifier.\n",
    "    - `created_at`: When the tweet was created (tweeted).\n",
    "    - `type`: Type of tweet: original tweet, replied (reply from another tweet), quoted or retweeted.\n",
    "    \n",
    "    \n",
    "- **users**: Pandas dataframe with users information. Columns:\n",
    "    - `user_id`: User's unique identifier.\n",
    "    - `username`: User's username.\n",
    "    - `name`: Name that is displayed on Twitter.\n",
    "    - `location`: User's location. In Twitter there is this field in the user's biography were users can specify their location. Maybe this location is where they were born, where they currently live or just a random place. However, many users do not really include a geographical location in there, some of them just write something else such as their pronouns. So this field do not necessary specifies a geographical location.\n",
    "\n",
    "\n",
    "- **places**: Pandas dataframe about places where a tweet was tweeted.\n",
    "    - `country`: Country where a tweet was tweeted.\n",
    "    - `full_name`: City, countrty where a tweet was tweeted.\n",
    "    - `geo.place_id`: Unique identifier of location.\n",
    "    - `name`: City name where a tweet was tweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes(json_tweets, today):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to create and organize different data into specific data frames.\n",
    "    \n",
    "    Inputs:\n",
    "        - json_tweets: A dictionary with tweets data.\n",
    "    \n",
    "    Outputs: \n",
    "        - tweets: Pandas dataframe with relevant information about tweets (to\n",
    "                  further perform text classification).\n",
    "                  \n",
    "        - users: Pandas dataframe with users information.\n",
    "        \n",
    "        - places (optional): Pandas dataframe about places where users tweeted. If not a \n",
    "                  single tweets contains the place where it was tweeted, then\n",
    "                  this dataframe will not be returned.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Create users dataframe\n",
    "    users = pd.json_normalize(json_tweets['includes']['users']).rename(columns = {\"id\":\"user_id\"})\n",
    "    \n",
    "    # Create df with tweet's data\n",
    "    tweets = pd.json_normalize(json_tweets['data']).rename(columns = {\"id\":\"tweet_id\", \n",
    "                                                                      \"geo.place_id\":\"geo_place_id\"})\n",
    "        \n",
    "    # Get tweet's type\n",
    "    tweets['type'] = tweets.referenced_tweets.apply(lambda x: x[0][\"type\"] if type(x) == list else None)\n",
    "    \n",
    "    # get referenced tweets ids\n",
    "    tweets[\"ref_tweet_id\"] = tweets.referenced_tweets.apply(lambda x: x[0]['id']\\\n",
    "                                                            if isinstance(x, list) else x)\n",
    "\n",
    "        \n",
    "    # Drop retweeted tweets and tweets with undefined anguage\n",
    "    #tweets = tweets[tweets[\"type\"] != \"retweeted\"].reset_index(drop = True)\n",
    "    tweets = tweets[tweets[\"lang\"] != \"und\"]\n",
    "        \n",
    "    # id to string\n",
    "    tweets[\"tweet_id\"] = tweets[\"tweet_id\"].astype(str)\n",
    "        \n",
    "    ## List of users in tweets dataframe to only \n",
    "    ## keep users from tweets dataframe\n",
    "    #user_list = tweets.author_id.unique()\n",
    "    #users = users.loc[users.user_id.isin(user_list)].reset_index(drop = True)\n",
    "        \n",
    "    # id to string\n",
    "    users[\"user_id\"] = users[\"user_id\"].astype(str)\n",
    "        \n",
    "    # from string to datetime\n",
    "    tweets[\"created_at\"] = pd.to_datetime(tweets[\"created_at\"], utc = True)\n",
    "        \n",
    "    # Not all users enable their location when tweeting, so\n",
    "    # we need to check if there are available locations for\n",
    "    # the tweets returned.\n",
    "    if \"places\" in json_tweets['includes'].keys():\n",
    "        # If the field exists, create a dataframe with the corresponding data\n",
    "        places = pd.json_normalize(json_tweets['includes']['places']).rename(columns = {\"id\":\"geo_place_id\"})\n",
    "            \n",
    "        # Drop cols\n",
    "        #tweets = tweets.drop(['referenced_tweets','edit_history_tweet_ids','geo.place_id'], axis = 1)\n",
    "        tweets = tweets.drop(['referenced_tweets','edit_history_tweet_ids'], axis = 1)\n",
    "        return tweets, users, places\n",
    "        \n",
    "    else:\n",
    "        # Drop cols\n",
    "        tweets = tweets.drop(['referenced_tweets','edit_history_tweet_ids'], axis = 1)\n",
    "        return tweets, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>lang</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>geo_place_id</th>\n",
       "      <th>type</th>\n",
       "      <th>ref_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1587626896996564993</td>\n",
       "      <td>2022-11-02 02:05:23+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>1154111700569853955</td>\n",
       "      <td>England’s provisional 55-man World Cup squad r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1587626894668705794</td>\n",
       "      <td>2022-11-02 02:05:23+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>1491848393613885443</td>\n",
       "      <td>RT @brfootball: Diego Forlán produced one of t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1587463431468322816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1587626891090964480</td>\n",
       "      <td>2022-11-02 02:05:22+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>1524886624978739200</td>\n",
       "      <td>RT @Bybit_NFT: 🔥 We are so excited to announce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1587482206607392769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                created_at  possibly_sensitive lang  \\\n",
       "0  1587626896996564993 2022-11-02 02:05:23+00:00               False   en   \n",
       "1  1587626894668705794 2022-11-02 02:05:23+00:00               False   en   \n",
       "2  1587626891090964480 2022-11-02 02:05:22+00:00               False   en   \n",
       "\n",
       "             author_id                                               text  \\\n",
       "0  1154111700569853955  England’s provisional 55-man World Cup squad r...   \n",
       "1  1491848393613885443  RT @brfootball: Diego Forlán produced one of t...   \n",
       "2  1524886624978739200  RT @Bybit_NFT: 🔥 We are so excited to announce...   \n",
       "\n",
       "  geo_place_id       type         ref_tweet_id  \n",
       "0          NaN       None                  NaN  \n",
       "1          NaN  retweeted  1587463431468322816  \n",
       "2          NaN  retweeted  1587482206607392769  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if we have tweet's location\n",
    "if \"places\" in search_tweet['includes'].keys():\n",
    "    main_tweets, main_users, main_places = create_dataframes(search_tweet, today)\n",
    "    \n",
    "else:\n",
    "    main_tweets, main_users = create_dataframes(search_tweet, today)\n",
    "    main_places = pd.DataFrame()\n",
    "    \n",
    "main_tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 9)\n",
      "(97, 4)\n"
     ]
    }
   ],
   "source": [
    "print(main_tweets.shape)\n",
    "print(main_users.shape)\n",
    "#print(main_places.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Request more data\n",
    "The `search_tweets` function was build to only resquest tweets one time, nevertheless, with the `next_token` parameter we can easily request more data. This parameter indicates that there are more \"pages\" or more results (tweets) that matches the query it was previously sent to Twitter API. If the `next_token` parameter is found in the returned dictionary, then it means there are more results than the ones first returned. If this parameter is missing, then there are no more tweets regarding this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b26v89c19zqg8o3fpzeme6kppzsjq765fvjcwvenluf0d\n",
      "2 b26v89c19zqg8o3fpzeme6kpoi8ax7kgivprx5rohcwvx\n",
      "3 b26v89c19zqg8o3fpzeme6kpoh5xczn1us9ozjo6xiqv1\n",
      "4 b26v89c19zqg8o3fpzeme6kpmzm1hkqfb4nkwplxf7531\n",
      "5 b26v89c19zqg8o3fpzeme6kpmyjgcuchobpscr0pns2v1\n",
      "6 b26v89c19zqg8o3fpzeme6kplgrx0ao1vlnweuse0thml\n",
      "7 b26v89c19zqg8o3fpzeme6kplfpf5qoxa1nsq63eav9ml\n",
      "8 b26v89c19zqg8o3fpzeme6kpjy5j99mwdqw9e83e2kr5p\n",
      "9 b26v89c19zqg8o3fpzeme6kpjwvbsptm9qa1fxagdi4jh\n",
      "10 b26v89c19zqg8o3fpzeme6kpif3kw9n6ze0no21bwkbr1\n",
      "11 b26v89c19zqg8o3fpzeme6kpgx4avw8egst546bi3oqd9\n",
      "12 b26v89c19zqg8o3fpzeme6kpgvue87h5mu4588q0xjfr1\n",
      "13 b26v89c19zqg8o3fpzeme6kf25chjbtfvpsm86hbzeum5\n",
      "14 b26v89c19zqg8o3fpzeme6kf24a1tn0i2gz1hjg96xm65\n",
      "15 b26v89c19zqg8o3fpzeme6kf0mq4v1d1qfriyunm0xxtp\n",
      "16 b26v89c19zqg8o3fpzeme6kf0lnmzf01e3sw7w7hvd6rh\n",
      "17 b26v89c19zqg8o3fpzeme6kez43lpkq5qc9y97w2e8y2l\n",
      "18 b26v89c19zqg8o3fpzeme6kez31aailvw193f7n4evtz1\n",
      "19 b26v89c19zqg8o3fpzeme6kexlh90ts3anabs7whjlxfh\n",
      "20 b26v89c19zqg8o3fpzeme6kexkes8927fhkj4l76thg1p\n",
      "21 b26v89c19zqg8o3fpzeme6kew2ut4bxsawlxqh3fzpif1\n",
      "22 b26v89c19zqg8o3fpzeme6kew1shpvo6jkzq27xy5ydml\n",
      "23 b26v89c19zqg8o3fpzeme6kew15btg732bvqpgenmoygt\n",
      "24 b26v89c19zqg8o3fpzeme6keuk8fd5dhjhph048w26xz1\n",
      "25 b26v89c19zqg8o3fpzeme6keuk0szcruc2fmy7b398osd\n",
      "26 b26v89c19zqg8o3fpzeme6keujdtjsfmmg6z3pjr9drp9\n",
      "27 b26v89c19zqg8o3fpzeme6keuj676aq6qpj4i3a2dpshp\n",
      "28 b26v89c19zqg8o3fpzeme6keuiqlicj777dxn8b143bel\n",
      "29 b26v89c19zqg8o3fpzeme6keuiiy2f7hkzy12wagrpsal\n",
      "30 b26v89c19zqg8o3fpzeme6ket1ee5wc50xtsspm6neflp\n",
      "31 b26v89c19zqg8o3fpzeme6ket0z6h712duckfbivqe6wt\n",
      "32 b26v89c19zqg8o3fpzeme6ket0z00brk6kb6rr8fdiikd\n",
      "33 b26v89c19zqg8o3fpzeme6ket0bzhq0r0bf9wy54egvwd\n",
      "34 b26v89c19zqg8o3fpzeme6ket04d4u9tipzvv2rec3j3x\n",
      "35 b26v89c19zqg8o3fpzeme6kerj7ejdmaqbbjgdefc6x6l\n",
      "36 b26v89c19zqg8o3fpzeme6keris4pnr9lutr2eweutbb1\n",
      "37 b26v89c19zqg8o3fpzeme6keriry7ktwzikwh65jw769p\n",
      "38 b26v89c19zqg8o3fpzeme6kerikarnkjaceks5d5xbhx9\n",
      "39 b26v89c19zqg8o3fpzeme6keri4ugx4igj6kwhr9i31x9\n",
      "40 b26v89c19zqg8o3fpzeme6kerhxbcjle3cdxww49o9y0t\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1, 16) # ~750\n",
    "# for i in range(1, 21): # ~1000\n",
    "for i in range(1, 41):\n",
    "    \n",
    "    # Check if there is a next token (another page)\n",
    "    # that matches the desired query\n",
    "    if 'next_token' in search_tweet['meta'].keys():\n",
    "        print(i, search_tweet[\"meta\"][\"next_token\"])\n",
    "\n",
    "        # Collect data from next token\n",
    "        new_tweets = search_tweets(query = query, next_token = search_tweet['meta']['next_token'])\n",
    "        search_tweet = new_tweets\n",
    "\n",
    "        # Check if any tweet has enabled the location,\n",
    "        # so we can create the places dataframe.\n",
    "        if \"places\" in search_tweet['includes'].keys():\n",
    "            tweets, users, places = create_dataframes(search_tweet, today = today)\n",
    "\n",
    "            # Append data to main tweets\n",
    "            main_tweets = main_tweets.append(tweets)\n",
    "            main_users = main_users.append(users)\n",
    "            main_places = main_places.append(places)\n",
    "\n",
    "            # Reset index\n",
    "            main_tweets = main_tweets.reset_index(drop = True)\n",
    "            main_users = main_users.reset_index(drop = True)\n",
    "            main_places = main_places.reset_index(drop = True)\n",
    "\n",
    "        # If any tweet has its location enabled, then only\n",
    "        # create the other two dataframes.\n",
    "        else: \n",
    "            tweets, users = create_dataframes(search_tweet, today = today)\n",
    "\n",
    "            # Append data to main tweets\n",
    "            main_tweets = main_tweets.append(tweets)\n",
    "            main_users = main_users.append(users)\n",
    "\n",
    "            # Reset index\n",
    "            main_tweets = main_tweets.reset_index(drop = True)\n",
    "            main_users = main_users.reset_index(drop = True)\n",
    "\n",
    "    # If there are not more results regarding the\n",
    "    # requested topic, then just stop requesting \n",
    "    # more data.\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4093, 11)\n",
      "(3789, 5)\n",
      "(11, 4)\n"
     ]
    }
   ],
   "source": [
    "print(main_tweets.shape)\n",
    "print(main_users.shape)\n",
    "print(main_places.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data locally\n",
    "if main_places.empty:\n",
    "    main_tweets.to_csv(f\"/home/bmartin/Documents/github_repos/tweets_nlp_visualizations/data/tweets/tweets_{today}.csv\",\n",
    "                       index = False)\n",
    "    main_users.to_csv(f\"/home/bmartin/Documents/github_repos/tweets_nlp_visualizations/data/users/users_{today}.csv\",\n",
    "                      index = False)\n",
    "    \n",
    "else:\n",
    "    main_tweets.to_csv(f\"/home/bmartin/Documents/github_repos/tweets_nlp_visualizations/data/tweets/tweets_{today}.csv\",\n",
    "                       index = False)\n",
    "    main_users.to_csv(f\"/home/bmartin/Documents/github_repos/tweets_nlp_visualizations/data/users/users_{today}.csv\",\n",
    "                      index = False)\n",
    "    main_places.to_csv(f\"/home/bmartin/Documents/github_repos/tweets_nlp_visualizations/data/places/places_{today}.csv\",\n",
    "                       index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clean data\n",
    "Before implementing the algorithm, we should start by cleaning and pre-processing our data, in this case, the papers csv is already loaded. The pre-processing phase includes the following steps and it's performed with help of the `PreProcessor` class:\n",
    "\n",
    "- **Remove noise:** Noise removal is about removing characters digits and pieces of text that can interfere with text analysis. Noise removal is one of the most essential text preprocessing steps.\n",
    "\n",
    "\n",
    "- **Normalize text:** Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. \n",
    "\n",
    "\n",
    "- **Tokenization:** Tokenization is a way of separating a piece of text into smaller units called tokens. In this case tokens are words (but can also be characters or subwords).\n",
    "\n",
    "\n",
    "- **Stemming:** Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words (known as a lemma).\n",
    "\n",
    "\n",
    "- **Lemmatization:** Lemmatization is a method responsible for grouping different inflected forms of words into the root form, having the same meaning. It is similar to stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    \n",
    "    def __init__(self, regex_dict = None):\n",
    "        \n",
    "        # creating classes\n",
    "        # stem\n",
    "        self.sb = nltk.stem.SnowballStemmer('english')\n",
    "        \n",
    "        # lemmatize\n",
    "        self.lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        \n",
    "        # translate\n",
    "        self.translator = Translator()\n",
    "        \n",
    "        # declare a default regex dict\n",
    "        self.default_regex_dict = {'goo[o]*d':'good', '2morrow':'tomorrow', 'b4':'before', 'otw':'on the way',\n",
    "                                   'idk':\"i don't know\", ':)':'smile', 'bc':'because', '2nite':'tonight',\n",
    "                                   'yeah':'yes', 'yeshhhhhhhh':'yes', ' yeeeee':'yes', 'btw':'by the way', \n",
    "                                   'fyi':'for your information', 'gr8':'great', 'asap':'as soon as possible', \n",
    "                                   'yummmmmy':'yummy', 'gf':'girlfriend', 'thx':'thanks','nowwwwwww':'now', \n",
    "                                   ' ppl ':' people ', 'yeiii':'yes'}\n",
    "        \n",
    "        # if no regex_dict defined by user, then use \n",
    "        # one by default. Else, concat two regex dicts\n",
    "        if regex_dict:            \n",
    "            self.regex_dict = {**regex_dict, **default_regex_dict}\n",
    "            \n",
    "        else:\n",
    "            self.regex_dict = self.default_regex_dict\n",
    "    \n",
    "    def translate_twt(self, pdf):\n",
    "    \n",
    "        \"\"\"\n",
    "        This function helps to translate a tweet from any \n",
    "        language to English.\n",
    "\n",
    "        Inputs:\n",
    "            - pdf: Pandas dataframe. This dataframe must have\n",
    "               the following columns:\n",
    "                - lang: Tweet's language.\n",
    "                - clean_tweet: Partially pre-processed tweet.\n",
    "\n",
    "        Outputs: Translated tweet from any language available \n",
    "                 in googletrans api to English.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the language of the tweet is either undefined or English\n",
    "        # to avoid translation.\n",
    "        if pdf[\"lang\"] == \"und\" or pdf[\"lang\"] == \"en\":\n",
    "            pdf[\"translated_tweet\"] = pdf[\"clean_tweet\"]\n",
    "\n",
    "        # Check if tweet is in Hindi. The code of Hindi language is \"hi\", but \n",
    "        # Twitter has defined the code as \"in\".\n",
    "        elif pdf[\"lang\"] == \"in\":\n",
    "            pdf[\"translated_tweet\"] = self.translator.translate(pdf[\"clean_tweet\"], src = \"hi\", dest = \"en\").text\n",
    "            \n",
    "        # Check if tweet is in Chinese. \n",
    "        # The api supports simplified and traditional chinese.\n",
    "        elif pdf[\"lang\"] == \"zh\":\n",
    "            pdf[\"translated_tweet\"] = self.translator.translate(pdf[\"clean_tweet\"], src = \"zh-cn\", dest = \"en\").text\n",
    "\n",
    "        # For any other language the translator should work just fine, so the\n",
    "        # api should work with the language detected by Twitter.\n",
    "        else:\n",
    "            try:\n",
    "                pdf[\"translated_tweet\"] = self.translator.translate(pdf[\"clean_tweet\"], src = pdf[\"lang\"], \n",
    "                                                                    dest = \"en\").text\n",
    "            except (TypeError, ValueError):\n",
    "                pdf[\"translated_tweet\"] = pdf[\"clean_tweet\"]\n",
    "                \n",
    "        return pdf[\"translated_tweet\"]\n",
    "\n",
    "    \n",
    "    def removeNoise(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to remove noise from strings. \n",
    "        \n",
    "        Inputs: A pandas dataframe with raw strings of length n.\n",
    "        \n",
    "        Output: A clean string where elements such as accented \n",
    "        words, html tags, punctuation marks, and extra white \n",
    "        spaces will be removed (or transform) if it's the case.\n",
    "        \"\"\"\n",
    "        \n",
    "        # to lower case\n",
    "        pdf[\"clean_tweet\"] = pdf.text.apply(lambda x: x.lower())\n",
    "        \n",
    "        # remove accented characters from string\n",
    "        # e.g. canción --> cancion\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: unidecode.unidecode(x))\n",
    "        \n",
    "        # remove html tags \n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.str.replace(r'<[^<>]*>', '', regex = True)\n",
    "        \n",
    "        # remove (match with) usernames | hashtags | punct marks | links\n",
    "        # punct marks = \",.':!?;\n",
    "        # do not remove: ' \n",
    "        # but remove: \"\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x:' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([-.,:_;])|(https?:\\/\\/.*[\\r\\n]*)\",\n",
    "                                                                            \"\", x).split()).replace('\"',''))\n",
    "                \n",
    "        # remove white spaces at the begining and at \n",
    "        # the end of a string\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: x.lstrip(' '))\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: x.rstrip(' '))\n",
    "        \n",
    "        # Translate tweet\n",
    "        pdf[\"clean_tweet\"] = pdf.apply(lambda x: self.translate_twt(x) \\\n",
    "                                       if pd.isnull(x.clean_tweet) == False else x, axis = 1)\n",
    "        \n",
    "        #pdf[\"clean_tweet\"] = pdf.apply(lambda x: self.translate_twt(x) \\\n",
    "        #                               if (pd.isnull(x.clean_tweet) == False \\\n",
    "        #                                   and x.clean_tweet != \"\") else x, axis = 1)    \n",
    "        \n",
    "        # normalize string\n",
    "        # normalize accented charcaters and other strange characters\n",
    "        # NFKD if there are accented characters (????\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: unicodedata.normalize('NFKC', x)\\\n",
    "                                                   .encode('ASCII', 'ignore').decode(\"utf-8\")\\\n",
    "                                                   if (pd.isnull(x) == False and x != \"\") else x)\n",
    "        \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def textNormalization(self, pdf):\n",
    "        \"\"\"\n",
    "        Function to normalize a string. \n",
    "        \n",
    "        Inputs: A pandas dataframe with strings (of length n) that \n",
    "        will be normalized. \n",
    "        \n",
    "        Outputs: A normalized string whitout noise, words in their\n",
    "        (expected) correct form and with no stopwords.\n",
    "        \"\"\"\n",
    "        \n",
    "        # remove noise first\n",
    "        pdf = self.removeNoise(pdf)\n",
    "\n",
    "        # expand contractions\n",
    "        # e.g. don't --> do not\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: contractions.fix(x))\n",
    " \n",
    "        # Normalize words\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.replace(self.regex_dict)\n",
    "                \n",
    "        # get English stopwords    \n",
    "        stop_words = stopwords.words('english')\n",
    "        stopwords_dict = Counter(stop_words)\n",
    "        \n",
    "        # remove stopwords from string\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: ' '.join([word for word in x.split()\n",
    "                                                                       if word not in stopwords_dict]))\n",
    "            \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def wordTokenize(self, pdf):\n",
    "        \"\"\"\n",
    "        Function to tokenize a string into words. Tokenization is a way \n",
    "        of separating a piece of text into smaller units called tokens.\n",
    "        In this case tokens are words (but can also be characters or \n",
    "        subwords).\n",
    "        \n",
    "        Inputs: A pandas dataframe with strings (of length n) that will be tokenized. \n",
    "        \n",
    "        Outputs: A list of tokenized words.\n",
    "        \"\"\"\n",
    "        # string normalized\n",
    "        #normalized = self.textNormalization(string)\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # Use word_tokenize method to split the string\n",
    "        # into individual words. By default it returns\n",
    "        # a list.\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: nltk.word_tokenize(x))        \n",
    "        \n",
    "        # Using isalpha() will help us to only keep\n",
    "        # items from the alphabet (no punctuation\n",
    "        # marks). \n",
    "        #pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [word for word in x if word.isalpha()])\n",
    "        \n",
    "        # Keep only unique elements\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: list(set(x)))\n",
    "\n",
    "        # return list of tokenized words by row\n",
    "        return pdf\n",
    "    \n",
    "    def phraseTokenize(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to tokenize a string into sentences. Tokenization is\n",
    "        a way of separating a piece of text into smaller units called\n",
    "        tokens. In this case tokens are phrases (but can also be words,\n",
    "        characters or subwords).\n",
    "        \n",
    "        Inputs: A string (of length n) that will be tokenized. \n",
    "        \n",
    "        Outputs: A list of tokenized sentences.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # Use sent_tokenize method to split the string\n",
    "        # into sentences. By default it returns a list.\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: nltk.sent_tokenize(x))   \n",
    "        \n",
    "        return pdf \n",
    "    \n",
    "    \n",
    "    def stemWords(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to stem strings. Stemming is the process of reducing\n",
    "        a word to its word stem that affixes to suffixes and prefixes \n",
    "        or to the roots of words (known as a lemma).\n",
    "        \n",
    "        Inputs: A raw string of length n.\n",
    "        \n",
    "        Output: Roots of each word of a given string.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # tokenized string (into words)\n",
    "        pdf = self.wordTokenize(data)\n",
    "            \n",
    "        # reduct words to its root    \n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [self.sb.stem(word) for word in x])\n",
    "        \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def lemmatizeWords(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to lemmatize strings. Lemmatization is a method \n",
    "        responsible for grouping different inflected forms of \n",
    "        words into the root form, having the same meaning. It is \n",
    "        similar to stemming.\n",
    "        \n",
    "        Inputs: A raw string of length n.\n",
    "        \n",
    "        Output: Roots of each word of a given string (with better\n",
    "        performance than in stemming).\n",
    "        \"\"\"\n",
    "        unw_chars = [\"(\", \")\", \"[\", \"]\"]\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # list of tokenized words (from string)\n",
    "        # Here it was decided to tokenize by words\n",
    "        # rather than by sentences due to it might\n",
    "        # be easier to find the correct roots\n",
    "        # of each word\n",
    "        pdf = self.wordTokenize(pdf)\n",
    "        \n",
    "        # lematize word from list of tokenized words\n",
    "        #lematized = [self.lemmatizer.lemmatize(word) for word in tokenized]\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [self.lemmatizer.lemmatize(word) \n",
    "                                                              for word in x if word not in unw_chars])\n",
    "        \n",
    "        return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3939, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1587626896996564993</td>\n",
       "      <td>England’s provisional 55-man World Cup squad r...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1587626894668705794</td>\n",
       "      <td>RT @brfootball: Diego Forlán produced one of t...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text lang\n",
       "0  1587626896996564993  England’s provisional 55-man World Cup squad r...   en\n",
       "1  1587626894668705794  RT @brfootball: Diego Forlán produced one of t...   en"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update values\n",
    "main_tweets.loc[main_tweets[\"possibly_sensitive\"] == False, \"possibly_sensitive\"] = 0\n",
    "main_tweets.loc[main_tweets[\"possibly_sensitive\"] == True, \"possibly_sensitive\"] = 1\n",
    "\n",
    "# get unique tweets ids from referenced tweets that were retweeted\n",
    "#ref_tweets = main_tweets.ref_tweet_id.unique().tolist()\n",
    "ref_tweets = main_tweets[main_tweets[\"type\"] == \"retweeted\"].ref_tweet_id.unique()\n",
    "\n",
    "# get unique tweets ids from original tweets\n",
    "og_tweets = main_tweets.tweet_id.unique().tolist()\n",
    "\n",
    "# get unique tweets that have been referenced\n",
    "# i.e., retweeted, quoted, etc\n",
    "both = [i for i in ref_tweets if i in og_tweets]\n",
    "\n",
    "# generate a new dataframe without tweets that have referenced other tweets\n",
    "# in order to avoid extra processing when cleaning the tweets\n",
    "sample_df = main_tweets[~main_tweets[\"ref_tweet_id\"].isin(og_tweets)]\n",
    "\n",
    "# keep necessary columns\n",
    "sample_df = sample_df[[\"tweet_id\", \"text\", \"lang\"]].reset_index(drop = True)\n",
    "print(sample_df.shape)\n",
    "sample_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1587626896996564993</td>\n",
       "      <td>England’s provisional 55-man World Cup squad r...</td>\n",
       "      <td>en</td>\n",
       "      <td>[welbeck, brighton, newcastle, pair, striker, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1587626894668705794</td>\n",
       "      <td>RT @brfootball: Diego Forlán produced one of t...</td>\n",
       "      <td>en</td>\n",
       "      <td>[2010, individual, rt, forlan, ever, mast, die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1587626891090964480</td>\n",
       "      <td>RT @Bybit_NFT: 🔥 We are so excited to announce...</td>\n",
       "      <td>en</td>\n",
       "      <td>[drop, 100, next, worth, rt, celebration, anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1587626890419904514</td>\n",
       "      <td>@AFC_Fazeel ENGLAND HAVE NO CHANCE IN THIS WOR...</td>\n",
       "      <td>en</td>\n",
       "      <td>[win, needed, cuptheir, league, idea, intensit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1587626877472374784</td>\n",
       "      <td>I’ve filled out the World Cup 2022 bracket for...</td>\n",
       "      <td>en</td>\n",
       "      <td>[win, bracket, filled, 20000, chance, cup, I, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1587626896996564993  England’s provisional 55-man World Cup squad r...   \n",
       "1  1587626894668705794  RT @brfootball: Diego Forlán produced one of t...   \n",
       "2  1587626891090964480  RT @Bybit_NFT: 🔥 We are so excited to announce...   \n",
       "3  1587626890419904514  @AFC_Fazeel ENGLAND HAVE NO CHANCE IN THIS WOR...   \n",
       "4  1587626877472374784  I’ve filled out the World Cup 2022 bracket for...   \n",
       "\n",
       "  lang                                        clean_tweet  \n",
       "0   en  [welbeck, brighton, newcastle, pair, striker, ...  \n",
       "1   en  [2010, individual, rt, forlan, ever, mast, die...  \n",
       "2   en  [drop, 100, next, worth, rt, celebration, anno...  \n",
       "3   en  [win, needed, cuptheir, league, idea, intensit...  \n",
       "4   en  [win, bracket, filled, 20000, chance, cup, I, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create class object\n",
    "pre_processor = PreProcessor()\n",
    "\n",
    "# Clean data and only keep the roots of each word.\n",
    "sample_df = pre_processor.lemmatizeWords(sample_df)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4093, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>lang</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>geo_place_id</th>\n",
       "      <th>type</th>\n",
       "      <th>ref_tweet_id</th>\n",
       "      <th>withheld.copyright</th>\n",
       "      <th>withheld.country_codes</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1587626896996564993</td>\n",
       "      <td>2022-11-02 02:05:23+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>1154111700569853955</td>\n",
       "      <td>England’s provisional 55-man World Cup squad r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[welbeck, brighton, newcastle, pair, striker, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1587626894668705794</td>\n",
       "      <td>2022-11-02 02:05:23+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>1491848393613885443</td>\n",
       "      <td>RT @brfootball: Diego Forlán produced one of t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1587463431468322816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2010, individual, rt, forlan, ever, mast, die...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                created_at possibly_sensitive lang  \\\n",
       "0  1587626896996564993 2022-11-02 02:05:23+00:00                  0   en   \n",
       "1  1587626894668705794 2022-11-02 02:05:23+00:00                  0   en   \n",
       "\n",
       "             author_id                                               text  \\\n",
       "0  1154111700569853955  England’s provisional 55-man World Cup squad r...   \n",
       "1  1491848393613885443  RT @brfootball: Diego Forlán produced one of t...   \n",
       "\n",
       "  geo_place_id       type         ref_tweet_id withheld.copyright  \\\n",
       "0          NaN       None                  NaN                NaN   \n",
       "1          NaN  retweeted  1587463431468322816                NaN   \n",
       "\n",
       "  withheld.country_codes                                        clean_tweet  \n",
       "0                    NaN  [welbeck, brighton, newcastle, pair, striker, ...  \n",
       "1                    NaN  [2010, individual, rt, forlan, ever, mast, die...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge main dataframe with sample df to get the clean tweet\n",
    "main_tweets = main_tweets.merge(sample_df[[\"tweet_id\", \"clean_tweet\"]],\n",
    "                                how = \"left\", on = \"tweet_id\")\n",
    "print(main_tweets.shape)\n",
    "main_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'created_at', 'possibly_sensitive', 'lang', 'author_id',\n",
      "       'text', 'geo_place_id', 'type', 'ref_tweet_id', 'clean_tweet'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "if \"withheld.copyright\" and \"withheld.country_codes\" in main_tweets.columns:\n",
    "    main_tweets = main_tweets.drop([\"withheld.copyright\", \"withheld.country_codes\"], axis = 1)\n",
    "    print(main_tweets.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>lang</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>geo_place_id</th>\n",
       "      <th>type</th>\n",
       "      <th>ref_tweet_id</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1587626896996564993</td>\n",
       "      <td>2022-11-02 02:05:23+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>1154111700569853955</td>\n",
       "      <td>England’s provisional 55-man World Cup squad r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[welbeck, brighton, newcastle, pair, striker, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1587626894668705794</td>\n",
       "      <td>2022-11-02 02:05:23+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>1491848393613885443</td>\n",
       "      <td>RT @brfootball: Diego Forlán produced one of t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1587463431468322816</td>\n",
       "      <td>[2010, individual, rt, forlan, ever, mast, die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1587626891090964480</td>\n",
       "      <td>2022-11-02 02:05:22+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>1524886624978739200</td>\n",
       "      <td>RT @Bybit_NFT: 🔥 We are so excited to announce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1587482206607392769</td>\n",
       "      <td>[drop, 100, next, worth, rt, celebration, anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1587626890419904514</td>\n",
       "      <td>2022-11-02 02:05:22+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>218768171</td>\n",
       "      <td>@AFC_Fazeel ENGLAND HAVE NO CHANCE IN THIS WOR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>1587004260704784388</td>\n",
       "      <td>[win, needed, cuptheir, league, idea, intensit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1587626877472374784</td>\n",
       "      <td>2022-11-02 02:05:18+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>1555016766774181888</td>\n",
       "      <td>I’ve filled out the World Cup 2022 bracket for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[win, bracket, filled, 20000, chance, cup, I, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                created_at possibly_sensitive lang  \\\n",
       "0  1587626896996564993 2022-11-02 02:05:23+00:00                  0   en   \n",
       "1  1587626894668705794 2022-11-02 02:05:23+00:00                  0   en   \n",
       "2  1587626891090964480 2022-11-02 02:05:22+00:00                  0   en   \n",
       "3  1587626890419904514 2022-11-02 02:05:22+00:00                  0   en   \n",
       "4  1587626877472374784 2022-11-02 02:05:18+00:00                  0   en   \n",
       "\n",
       "             author_id                                               text  \\\n",
       "0  1154111700569853955  England’s provisional 55-man World Cup squad r...   \n",
       "1  1491848393613885443  RT @brfootball: Diego Forlán produced one of t...   \n",
       "2  1524886624978739200  RT @Bybit_NFT: 🔥 We are so excited to announce...   \n",
       "3            218768171  @AFC_Fazeel ENGLAND HAVE NO CHANCE IN THIS WOR...   \n",
       "4  1555016766774181888  I’ve filled out the World Cup 2022 bracket for...   \n",
       "\n",
       "  geo_place_id        type         ref_tweet_id  \\\n",
       "0          NaN        None                  NaN   \n",
       "1          NaN   retweeted  1587463431468322816   \n",
       "2          NaN   retweeted  1587482206607392769   \n",
       "3          NaN  replied_to  1587004260704784388   \n",
       "4          NaN        None                  NaN   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  [welbeck, brighton, newcastle, pair, striker, ...  \n",
       "1  [2010, individual, rt, forlan, ever, mast, die...  \n",
       "2  [drop, 100, next, worth, rt, celebration, anno...  \n",
       "3  [win, needed, cuptheir, league, idea, intensit...  \n",
       "4  [win, bracket, filled, 20000, chance, cup, I, ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tweet in both:\n",
    "    # get the clean tweet from the original tweet\n",
    "    clean_tweet = str(list(main_tweets.loc[(main_tweets[\"tweet_id\"] == tweet), \"clean_tweet\"])[0])\n",
    "    \n",
    "    # assign the clean tweet to the \"clean_tweet\" column\n",
    "    main_tweets.loc[(main_tweets[\"ref_tweet_id\"] == tweet), \"clean_tweet\"] = clean_tweet\n",
    "\n",
    "# delete rows where text couldn't be cleanned\n",
    "main_tweets = main_tweets.dropna(subset = [\"clean_tweet\"]).shape\n",
    "main_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data locally\n",
    "main_tweets.to_csv(f'/home/bmartin/Documents/github_repos/tweets_nlp_visualizations/data/clean_tweets/clean_tweets_{today}.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import ast\n",
    "main_tweets[\"clean_tweet\"] = main_tweets.clean_tweet.apply(lambda x: ast.literal_eval(x)\\\n",
    "                                                           if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "char = '✅'\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
